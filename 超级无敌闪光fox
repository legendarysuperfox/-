import torch
my_tensor = torch. tensor([[1, 2, 3], [4, 5, 6]], dtype=torch. float32,device=device, requires_grad=True)

print (my_tensor)
print (my_tensor.dtype)
print ( my_tensor.device)
print (my_tensor.shape)
print (my_tensor.requires_grad)

# Other common initialization methods
x = torch.empty(size = (3, 3))
X = torch.zeros((3, 3))
X = torch.rand((3, 3))
X = torch.ones((3, 3))
x = torch.eye(5, 5) # I,eye
X =torch.arange (start=0 ,end=5, step=1 )
X =torch.linspace(start=0.1, end=1, steps=10)

x = torch.empty(size=(1, 5)) .normal_ (mean=0, std=1)
x = torch. empty(size=(1, 5)).uniform_ (0, 1)
x = torch. diag(torch. ones(3))

# How to initialize and convert tensors to other types (int, float, double)tensor = torch. arange(4)

print (my_tensor. bool()) # boolean True/False
print (my_tensor.short()) # int16
print (my_tensor.long()) #int64 ( Important)
print (my_tensor.half()) # float16
print (my_tensor.float()) #float32 ( Important)
print (my_tensor . double()) #float64

# Array to Tensor conversion and vice-versa
import numpy as np
np_array = np.zeros((5, 5))
tensor = torch.from_numpy(np_array)
np_array_back = tensor.numpy()`



import torch

# ================================================================
# Tensor Math & Comparison Operations
# ================================================================
x = torch.tensor([1, 2, 3])
y = torch. tensor([9, 8,7])

# Addition
z1 = torch. empty(3)
torch.add(x, y, out=z1)

z2 = torch.add(x, y)
z = x+y

# SubtractionZ=X-y
z = x-y

# Division
z = torch.true_divide(x, y)

# inplace operations
t = torch.zeros(3)
t.add_(x)

t += x # t=t+X

# Exponentiation
z = x.pow(2)
z = x**2

# Simple comparison
z = x>0
z = x<0

# Matrix Multiplication
x1 = torch.rand((2, 5))
x2 = torch.rand((5, 3))
x3 = torch. mm(x1, x2) # 2x3
x3 = x1.mm(x2)

# matrix exponentiation

matrix_exp = torch. rand(5, 5)
print (matrix_exp.matrix_power(3))

# element wise mult.
z = x * y
print(z)

# dot product
z = torch.dot(x, y)
print(z)

# Batch Matrix Multiplication
batch = 32

n=10
m=20
p=30

tensor1 = torch.rand((batch, n, m))

tensor2 = torch.rand( (batch, m,p))

out_bmm = torch.bmm(tensor1, tensor2) # (batch, n, p)

# Example of Broadcasting
x1 = torch.rand((5,5))
x2 = torch.rand((1,5))

z = x1- x2
z = x1 ** x2

# Other useful tensor operations
sum_x = torch.sum(x, dim=0)
values, indices = torch.max(x, dim=0)
values, indices = torch.min(x, dim=0)
abs_x = torch.abs(x)
z = torch. argmax(x, dim=0)
z = torch. argmin(x, dim=0)
mean_x = torch.mean(x.float(), dim=0)
z = torch.eq(x, y)

sorted_y, indices = torch.sort(y, dim=0, descending=False)

Z = torch.clamp(x, min=0)

x = torch. tensor([1,0,1,1,1], dtype=torch.bool)
z = torch.any(x)
z = torch.all(x)


import numpy as np

a = np.array([4,5,6])
print(type(a))
print(a.shape)
print(a[0])

print('-----------------------------------------------------------')

b=np.array([[4,5,6],[1,2,3]])
print(b.shape)
print(b[0,0])
print(b[0,1])
print(b[1,1])

print('-----------------------------------------------------------')

a=np.zeros((3,3),dtype=int)
b=np.ones((4,5),dtype=int)
c=np.identity(4)
d=np.random.rand(3,2)

print('-----------------------------------------------------------')

a=np.arange(1,13).reshape(3,4)
print(a)
print(a[2,3])
print(a[0,0])

print('-----------------------------------------------------------')

b=a[0:2,1:3]
print(b)
print(b[0,0])

print('-----------------------------------------------------------')

c=a[1:3,:]
print(c)
print(c[0][-1])

print('-----------------------------------------------------------')

a = np.array([[1,2],[3,4][5,6]])
print(a[[0,1,2],[0,1,0]])

print('-----------------------------------------------------------')

a = np.arange(1,13).reshape(4,3)
b = np.array([0,2,0,1])
print(a[[np.arange(4),b]])

print('-----------------------------------------------------------')

a[[np.arange(4),b]]+=10
print(a[[np.arange(4),b]])

print('-----------------------------------------------------------')

x=np.array([[1,2]])
print(x.dtype)

print('-----------------------------------------------------------')

x=np.array([1.0,2.0])
print(x.dtype)

print('-----------------------------------------------------------')

x=np.array([[1,2],[3,4];dtype=np.float64
y=np.array([[5,6],[7,8]);type=np.float64
print(x+y)
print(np.add(x,y))
print('-----------------------------------------------------------')
print(x-y)
print(np.subtract(x,y))
print('-----------------------------------------------------------')
print(x*y)
print(np.multiply(x,y))
print(np.dot(x,y)) # 矩阵相乘
print('-----------------------------------------------------------')
print(x/y)
print(np.divide(x,y))
print('-----------------------------------------------------------')
print(np.sqrt(x))
print('-------------------------------------------------')


import numpy as np
import torch
import torch.nn as nn
def forward(x,w1,w2):
    net1 = nn.Linear(2,2)
    net1.weight.data = w1
    net1.bias.data = torch.Tensor([0])
    h = net1(x)
    h= torch.sigmoid(h)

    net2 = nn.Linear(2,2)
    net2.weight.data = w2
    net2.bias.data = torch.Tensor([0])
    o = net2.forward(x)
    o = torch.sigmoid(o)
    return o


if __name__=='__main__':
    x = torch.tensor([0.5, 0.3])
    w1 = torch.tensor([[0.2, 0.5], [-0.4, 0.6]])
    w2 = torch.tensor([[0.1, -0.3], [-0.5, 0.8]])
    output = forward(x,w1,w2)
    print('最终的输出值是：',output)


import numpy as np
import torch
import torch.nn as nn
def forward(x,w1,w2):
    net1 = nn.Linear(2,2)
    net1.weight.data = w1
    net1.bias.data = torch.Tensor([0])
    h = net1(x)
    h= torch.sigmoid(h)

    net2 = nn.Linear(2,2)
    net2.weight.data = w2
    net2.bias.data = torch.Tensor([0])
    o = net2.forward(x)
    o = torch.sigmoid(o)
    return o


if __name__=='__main__':
    x = torch.tensor([0.5, 0.3])
    w1 = torch.tensor([[0.2, 0.5], [-0.4, 0.6]])
    w2 = torch.tensor([[0.1, -0.3], [-0.5, 0.8]])
    output = forward(x,w1,w2)
    print('最终的输出值是：',output)


import torch
import torch.nn as nn

class Net(nn.Module):
    def __init__(self,input_size,hidden_size,output_size) -> None:
        super().__init__()
        self.fc1 = nn.Linear(input_size,hidden_size)
        self.sigmoid = torch.nn.Sigmoid() #torch.sigmoid
        self.fc2 = nn.Linear(hidden_size,output_size) #

    def forward(self,x,w1,w2):
        self.fc1.weight.data = w1
        self.fc1.bias.data = torch.Tensor([0])
        h = self.fc1(x)
        h = self.sigmoid(h)

        self.fc2.weight.data = w2
        self.fc2.bias.data = torch.Tensor([0])
        o = self.fc2(h)
        o = self.sigmoid(o)
        return o

if __name__=='__main__':
    x = torch.tensor([0.5, 0.3])
    w1 = torch.tensor([[0.2, 0.5], [-0.4, 0.6]])
    w2 = torch.tensor([[0.1, -0.3], [-0.5, 0.8]])

    net = Net(2,2,2)
    output = net(x,w1,w2)
    # net.forward(x,w1,w2)
    print('最终的输出值为：',output)



import torch
import torch.nn as nn
class Net(nn.Module):
    def __init__(self,input_size,hidden_size,output_size) -> None:
        super().__init__()
        self.fc1 = nn.Linear(input_size,hidden_size)
        self.sigmoid = torch.nn.Sigmoid()
        self.fc2 = nn.Linear(hidden_size,output_size)
    def forward(self,x,w1,w2):
        self.fc1.weight.data = w1
        self.fc1.bias.data = torch.Tensor([0])
        out = self.fc1(x)
        out = self.sigmoid(out)

        self.fc2.weight.data=w2
        self.fc2.bias.data = torch.Tensor([0])
        output = self.fc2(out)
        output=self.sigmoid(output)
        return output

if __name__=='__main__':
    x = torch.tensor([0.5, 0.3])
    y = torch.tensor([0.23, 0.07])
    w1 = torch.tensor([[0.2, 0.5], [-0.4, 0.6]])
    w2 = torch.tensor([[0.1, -0.3], [-0.5, 0.8]])
    net = Net(2,2,2)

    loss = nn.MSELoss()  #定义损失函数
    optimizer = torch.optim.SGD(params=net.parameters(),lr=1e-2) #定义优化器

    for i in range(1000):
        output = net(x,w1,w2)
        loss_fn = loss(output,y)
        optimizer.zero_grad()
        loss_fn.backward()
        optimizer.step()
        print('损失函数的变化情况',i,loss_fn)
    print(output)
